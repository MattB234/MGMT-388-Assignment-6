import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Question 1
file_path = "/Users/paulwilliams/Downloads/Lab 6 - Boston Housing Data.csv"
dataset = pd.read_csv(file_path)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
print(X)
print(y)

print(dataset.shape)

print(dataset.head())

print(dataset.info())

print(dataset.describe())

print(dataset.isnull().sum())

dataset = dataset.fillna(dataset.mean())

We used mean imputation to replace the missing values in the six columns because it affected a small portion of the data and spanned across multiple columns that are important.

print(dataset.isnull().sum())

# Question 2
sns.histplot(dataset['MEDV'], bins=30, kde=True)
plt.title('Distribution of Median House Value')
plt.xlabel('MEDV')

The histogram shows that the median value of owner occupied homes in Boston is on average 20,000 to 25,000 dollars.

sns.histplot(dataset['AGE'], bins=30, kde=True)
plt.title('Distribution of Age')
plt.xlabel('Age')

This histogram visualizes the data we discovered when we printed the data set's description. It supports the high mean of age and its mode being around 100. To understand, it is essential to understand the variable Age itself. Age represents the proportion of owner-occupied homes built before 1940 so our histogram shows that most of the neighborhoods contains older homes built before 1940.

plt.figure(figsize=(12, 10))
sns.heatmap(dataset.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')

Although we have not focused much on such correlation heat maps, it is very usual for visualizing this data especially the correlation between different variables. As we can see Tax and RAD (property tax rate and index of accessibility to radial highways) have a correlation coefficient of 0.91 since due to public planning and policy in Boston, houses near highways have high tax rates. Also, tax and industry usage and separately median value and average number of rooms are naturally close to 1 as well. Industry usage and distance to employment centers, and median value and percent lower status of the population are both highly negatively correlated, close to -1. 

sns.scatterplot(x='AGE', y='MEDV', data=dataset)
plt.title('AGE vs MEDV')
plt.xlabel('% of Homes Built Before 1940 (AGE)')
plt.ylabel('Median House Value (MEDV)')

We feel it was important to visualize the relationship between the age of Boston's houses and their median value to see if more aged houses added or subtracted value. As we can see there is not a strong relationship present. However, with the results from the previous heatmap and this scatter plot, there is a sizable enough negative relationship between the two around r = -0.4 so older houses tend to lose value.

sns.histplot(dataset['LSTAT'], bins=30, kde=True)
plt.title('Distribution of Lower Status Population (%)')
plt.xlabel('LSTAT')

Lastly, we want to visualize LSTAT in a histogram in visualize Boston's socioeconomic distribution. The histogram is heavily skewed right with the majority of values falling around 10-15, so most Boston neighborhoods have a low percentage of lower status population. Most homes are in safer areas and as we saw previously, those that are not have lower median value. 

# Question 3
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assigns Median House Value as the target variable
X = dataset.drop('MEDV', axis=1)
y = dataset['MEDV']

# Z-Score scaling
standard_scaler = StandardScaler()
X_scaled_data = standard_scaler.fit_transform(X)
print(X_scaled_data)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled_data, y, test_size = 0.2, random_state = 0)
print(X_test)
print(y_test)

# Question 4
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# Creation of linear regression model
lr = LinearRegression()

# Training the newly created model using the training dataset
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

# Evaluation of model's performance using Mean Squared Error and R-squared
print( "Mean Squared Error:", mean_squared_error(y_test,y_pred))
print( "R^2 Score:", r2_score(y_test,y_pred))

# Question 5
# Prediction of the Test set
y_pred = lr.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(-1,1), y_test.to_numpy().reshape(-1, 1)), axis=1))

# Evaluation of model's performance using Mean Squared Error and R-squared
print( "Mean Squared Error:", mean_squared_error(y_test,y_pred))
print( "R^2 Score:", r2_score(y_test,y_pred))

# Scatterplot displaying the actual (y_test) vs. predicted (y_pred) housing prices
plt.scatter(y_test, y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '-', color ='orange')
plt.grid(True)
plt.xlabel("Actual MEDV")
plt.ylabel("Predicted MEDV")
plt.title("Actual vs. Predicted MEDV")
plt.show()

With the R^2 value being 0.57, 57% of the variability in housing prices is explained by our model. 
The MSE value being 34.99 shows the average squared difference between the predicted and actual median housing prices. The square root of the MSE value is 5.915. Since MEDV is measured in $1000s, on average, the model is off by $5915 when predicting the housing prices of Boston.
